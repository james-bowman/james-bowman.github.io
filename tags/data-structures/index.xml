<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Structures on James Bowman</title>
    <link>http://www.jamesbowman.me/tags/data-structures/index.xml</link>
    <description>Recent content in Data Structures on James Bowman</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <copyright>Creative Commons Attribution 4.0 International License (CC BY)</copyright>
    <atom:link href="http://www.jamesbowman.me/tags/data-structures/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Optimising algorithms in Go for machine learning - Part 2: Sparse matrix formats</title>
      <link>http://www.jamesbowman.me/post/optimising-machine-learning-algorithms-part2/</link>
      <pubDate>Fri, 09 Jun 2017 16:45:40 +0100</pubDate>
      
      <guid>http://www.jamesbowman.me/post/optimising-machine-learning-algorithms-part2/</guid>
      <description>

&lt;p&gt;This is the second in a series of blog posts sharing my experiences working with algorithms and data structures for machine learning.  These experiences were gained whilst building out the &lt;a href=&#34;http://github.com/james-bowman/nlp&#34;&gt;nlp project&lt;/a&gt; for &lt;a href=&#34;http://www.jamesbowman.me/post/semantic-analysis-of-webpages-with-machine-learning-in-go/&#34;&gt;LSA (Latent Semantic Analysis)&lt;/a&gt; of text documents.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;http://www.jamesbowman.me/post/optimising-machine-learning-algorithms/&#34;&gt;Part 1 of this series&lt;/a&gt;, I explored alternative approaches for representing and applying &lt;a href=&#34;http://www.jamesbowman.me/post/semantic-analysis-of-webpages-with-machine-learning-in-go/#tf-idf-term-frequency-inverse-document-frequency&#34;&gt;TF-IDF&lt;/a&gt; transforms for weighting term frequencies across document corpora.  We tested the approaches using Go&amp;rsquo;s inbuilt benchmark functionality and found that our optimisations materially improved not just memory consumption but also performance (reducing memory consumption and processing time from 7 GB and 41 seconds to 250 KB and 0.8 seconds respectively).  In this blog post I shall explore other areas for optimisation, seeking to further reduce memory consumption and processing time.&lt;/p&gt;

&lt;h2 id=&#34;matrix-sparsity&#34;&gt;Matrix Sparsity&lt;/h2&gt;

&lt;p&gt;Machine learning applications typically model entities as vectors of numerical features so that they may be compared and analysed quantitively.  These vectors can be considered collectively as a matrix.  In the case of &lt;a href=&#34;http://www.jamesbowman.me/post/semantic-analysis-of-webpages-with-machine-learning-in-go/&#34;&gt;Latent Semantic Analysis (LSA)&lt;/a&gt; as implemented in the &lt;a href=&#34;http://github.com/james-bowman/nlp&#34;&gt;nlp project&lt;/a&gt;, documents are the entities and the words/terms they contain are the features.  The elements within the matrix represent the frequency that each word appears in the associated document.  &lt;a href=&#34;http://www.jamesbowman.me/post/semantic-analysis-of-webpages-with-machine-learning-in-go/&#34;&gt;For more detailed explanation of term document matrices and LSA please refer to this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The vocabulary of words occuring within a modestly sized document corpus of 3,000 documents could be hundreds of thousands of words.  However, each document probably only contains a couple of hundred unique terms.  To represent such a corpus as numerical feature vectors would therefore require a ~200,000 x 3,000 matrix (terms x documents) with 99% of the elements containing zeros.  Storing all the elements of such a matrix, using 64 bits per element, would require over 4GB of memory.&lt;/p&gt;

&lt;p&gt;Thankfully, there are data structures and algorithms specifically designed for dealing with such sparse matrices that capitalise on the sparsity of the matrix by only storing the non-zero values.  This reduces the memory/storage requirements and processing effort to represent and process the matrix.&lt;/p&gt;

&lt;h2 id=&#34;sparse-matrix-formats&#34;&gt;Sparse Matrix Formats&lt;/h2&gt;

&lt;p&gt;Sparse matrix data structures can effectively be divided into 3 main categories:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Creational&lt;/strong&gt; - Sparse matrix formats suited to construction and random access updates of matrices.  Matrix formats in this category include &lt;a href=&#34;#dok-dictionary-of-keys-format&#34;&gt;DOK (Dictionary Of Keys)&lt;/a&gt; and &lt;a href=&#34;#coo-coordinate-format&#34;&gt;COO (COOrdinate)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Operational&lt;/strong&gt; - Sparse matrix formats suited to arithmetic operations e.g. multiplication or other operations requiring sequential access to elements.  Matrix formats in this category include &lt;a href=&#34;#csr-compressed-sparse-row-format&#34;&gt;CSR (Compressed Sparse Row)&lt;/a&gt; and &lt;a href=&#34;#csc-compressed-sparse-column-format&#34;&gt;CSC (Compressed Sparse Column)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Specialised&lt;/strong&gt; - Specialised matrix formats optimised for specific sparsity patterns.  Matrix formats in this category include &lt;a href=&#34;#dia-diagonal-format&#34;&gt;DIA (DIAgonal)&lt;/a&gt; for efficiently storing and manipulating symmetric diagonal matrices.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A common practice is to construct sparse matrices using a creational format e.g. DOK or COO and then convert them to an operational format e.g. CSR for arithmetic operations.&lt;/p&gt;

&lt;h3 id=&#34;1-creational-formats&#34;&gt;1. Creational Formats&lt;/h3&gt;

&lt;h4 id=&#34;dok-dictionary-of-keys-format&#34;&gt;DOK (Dictionary Of Keys) format&lt;/h4&gt;

&lt;p&gt;DOK format uses a dictionary/map data structure as its backing store, mapping row/column pairs (i, j) to matrix elements containing non-zero values.  Only non-zero values and their row/column index pairs are stored so any items missing from the map are assumed to be zero.  Using a hash map as the underlying data structure means random access (both reads and writes) is relatively fast (&lt;em&gt;O(1)&lt;/em&gt;) but sequential iteration over the elements is relatively slow making this format a good choice for incrementally constructing or updating a matrix but poor for arithmetic operations.&lt;/p&gt;

&lt;h4 id=&#34;coo-coordinate-format&#34;&gt;COO (COOrdinate) format&lt;/h4&gt;

&lt;p&gt;Also known as Triplet format, the COO format stores the row and column indices of non-zero values along with the values themselves.  Each row index, column index and data value tuple is stored in 3 respective slices such that &lt;code&gt;element(row[i], column[i]) = value[i]&lt;/code&gt;.  Since the slices are unordered and duplicate elements are allowed, appending non-zero elements to the end of the slices is very fast (&lt;em&gt;O(1)&lt;/em&gt;).  However, this also means that random access reads of elements is relatively slow (&lt;em&gt;O(n)&lt;/em&gt;) and sequential iteration can also be slow (sorting the slices can improve access times).  These characteristics make this matrix format a good choice for initial construction of a matrix or as an intermediate format for converting to CSR format but poor for arithmetic operations (assuming the slices are unsorted).&lt;/p&gt;

&lt;h3 id=&#34;2-operational-formats&#34;&gt;2. Operational Formats&lt;/h3&gt;

&lt;h4 id=&#34;csr-compressed-sparse-row-format&#34;&gt;CSR (Compressed Sparse Row) format&lt;/h4&gt;

&lt;p&gt;Also known as CRS (Compressed Row Storage), this format is similar to COO above except that the row index slice is compressed.  Specifically, the row index slice stores the cumulative count of non-zero elements in each row such that &lt;code&gt;row[i]&lt;/code&gt; contains the index into both &lt;code&gt;column[]&lt;/code&gt; and &lt;code&gt;data[]&lt;/code&gt; of the first non-zero element of row &lt;code&gt;i&lt;/code&gt;.  Thus ranging across data from &lt;code&gt;data[row[i]]&lt;/code&gt; to &lt;code&gt;data[row[i+1]-1]&lt;/code&gt; will yield all values from row &lt;code&gt;i&lt;/code&gt;.  For &lt;a href=&#34;https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_.28CSR.2C_CRS_or_Yale_format.29&#34;&gt;a more detailed explanation of CSR sparse matrix format please refer here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Relative to COO, the compression reduces storage requirements, allows faster random access reads of elements and row slicing but means making changes to the sparsity pattern is very slow (changing a zero value to non-zero).  These characteristics make this format a poor choice for random access updates, passable for random access reads and good for arithmetic operations.&lt;/p&gt;

&lt;h4 id=&#34;csc-compressed-sparse-column-format&#34;&gt;CSC (Compressed Sparse Column) format&lt;/h4&gt;

&lt;p&gt;Also known as CCS (Compressed Column Storage), this format is identical to CSR above except that the column index slice is compressed rather than the row index slice as with CSR.  The result is that CSC stores values in column major order rather than row major order as with CSR.  CSC can be thought of as a natural transpose of CSR.&lt;/p&gt;

&lt;h3 id=&#34;3-specialised-formats&#34;&gt;3. Specialised Formats&lt;/h3&gt;

&lt;h4 id=&#34;dia-diagonal-format&#34;&gt;DIA (DIAgonal) format&lt;/h4&gt;

&lt;p&gt;DIA is a specialised format for storing symmetric diagonal matrices.  Symmetric diagonal matrices are square shaped and so have the same number of rows and columns, with only the elements along the diagonal (top left to bottom right) containing non-zero values.  The DIA format takes advantage of this fact by only storing the diagonal values which it stores in a single slice such that &lt;code&gt;element(i, j) = value[j] or 0 if i != j&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;putting-it-into-practice&#34;&gt;Putting it into Practice&lt;/h2&gt;

&lt;p&gt;Our implementation of Latent Semantic Analysis in the nlp project comprises 3 steps as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#1-feature-extraction-vectorisation&#34;&gt;&lt;strong&gt;Feature Extraction/Vectorisation&lt;/strong&gt;&lt;/a&gt; - Converting raw text documents into vectors of numerical features to create a term document matrix&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;#2-tf-idf-weighting&amp;quot; }}&#34;&gt;&lt;strong&gt;TF/IDF Weighting&lt;/strong&gt;&lt;/a&gt; - Extracting weighting values based upon values in the term document matrix and then applying them to the matrix.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;#3-truncated-singular-value-decomposition&amp;quot; }}&#34;&gt;&lt;strong&gt;Truncated Singular Value Decomposition&lt;/strong&gt;&lt;/a&gt; - reducing the dimensionality (to the top k most significant dimensions) of the weighted term document matrix.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Currently, each of these 3 steps outputs a dense matrix.  In addition, the TF-IDF weightings extracted and applied in step 2 are stored as a dense matrix to be multiplied with the term document matrix.&lt;/p&gt;

&lt;h3 id=&#34;1-feature-extraction-vectorisation&#34;&gt;1. Feature Extraction/Vectorisation&lt;/h3&gt;

&lt;p&gt;Step 1 constructs a matrix incrementally based upon the terms encountered whilst parsing the document corpus.  We suspect the matrix will be very sparse as most documents will only contain a couple of hundred unique terms from a corpus wide vocabulary of hundreds of thousands of terms.  The current dense matrix implementation is therefore very wasteful storing all of the zero values in addition to the non-zero values.  A creational sparse matrix format is probably the most appropriate format to use for this step.  Specifically, a DOK (Dictionary Of Keys) format will be most suited in this case as the matrix is constructed incrementally.&lt;/p&gt;

&lt;p&gt;Here is a code snippet showing the current feature extraction using a Dense format matrix.&lt;/p&gt;
func (v *CountVectoriser1) Transform(docs ...string) (*mat64.Dense, error) {
	mat := mat64.NewDense(len(v.Vocabulary), len(docs), nil)

	for d, doc := range docs {
		words := v.tokenise(doc)

		for _, word := range words {
			i, exists := v.Vocabulary[word]

			if exists {
				mat.Set(i, d, mat.At(i, d)+1)
			}
		}
	}
	return mat, nil
}

&lt;p&gt;And here is the same code modified to use a Dictionary Of Keys (DOK) sparse format matrix.&lt;/p&gt;
func (v *DOKCountVectoriser1) Transform(docs ...string) (*sparse.DOK, error) {
	mat := sparse.NewDOK(len(v.Vocabulary), len(docs))

	for d, doc := range docs {
		words := v.tokenise(doc)

		for _, word := range words {
			i, exists := v.Vocabulary[word]

			if exists {
				mat.Set(i, d, mat.At(i, d)+1)
			}
		}
	}
	return mat, nil
}

&lt;p&gt;As you can see there is little difference to the code beyond the type of matrix used.  To test the two versions I used Go&amp;rsquo;s built in benchmark functionality and a subset of the 20 Newsgroups dataset as used in Scikit Learn.  The subset of the dataset used for this benchmark comprises 2,001 documents representing a vocabulary of 33,552 unique terms.  This results in a a term document matrix of size 33,552 x 2,001 with only 385,944 non-zero values (99.5% of the matrix elements contain zero values).  Here are the two benchmark functions.&lt;/p&gt;
// Benchmark feature extraction vectorisation into Dense vs Sparse matrices

// Baseline Dense matrix vectorisation
func BenchmarkDenseCountVectoriserTransform(b *testing.B) {
	files := Load(&#34;sci.space&#34;, &#34;sci.electronics&#34;)

	vect := NewCountVectoriser1(false)
	vect.Fit(files...)

	b.ResetTimer()
	for n := 0; n &lt; b.N; n++ {
		vect.Transform(files...)
	}
}

// Benchmark DOK Sparse matrix vectorisation
func BenchmarkDOKCountVectoriserTransform(b *testing.B) {
	files := Load(&#34;sci.space&#34;, &#34;sci.electronics&#34;)

	vect := NewDOKCountVectoriser1(false)
	vect.Fit(files...)

	b.ResetTimer()
	for n := 0; n &lt; b.N; n++ {
		vect.Transform(files...)
	}
}

&lt;p&gt;The results are shown below.  It should be clear from the results that the new Sparse matrix implementation is slightly faster than the Dense matrix based implementation at 0.8 seconds vs 1.1 but most importantly uses considerably less memory (80 MB vs the 500 MB used by the Dense implementation).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jamess-MacBook-Pro:nlpbench jbowman$ go test -bench=CountVectoriserTransform -benchmem
BenchmarkDenseCountVectoriserTransform-4  1	  1149735130 ns/op	 588033600 B/op	  688868 allocs/op
BenchmarkDOKCountVectoriserTransform-4    2	  849252150 ns/op	 83780632 B/op	  712011 allocs/op
PASS
ok  	github.com/james-bowman/nlpbench	6.640s
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-tf-idf-weighting&#34;&gt;2. TF/IDF Weighting&lt;/h3&gt;

&lt;p&gt;In &lt;a href=&#34;http://www.jamesbowman.me/post/optimising-machine-learning-algorithms/&#34;&gt;part 1 of this series&lt;/a&gt;, we identified the TF-IDF weightings matrix to be a symmetric diagonal matrix and so switched to storing it as a simple slice containing just the diagonal values.  This resulted in material improvements to both storage requirements and processing time.  We will now switch back to using a matrix for the TF-IDF weightings but this time, rather than using a dense matrix format as before, we will use the appropriate DIAgonal sparse matrix format.&lt;/p&gt;

&lt;p&gt;Step 2, takes the term document matrix constructed during step 1, extracts weighting values and stores them in a DIAgonal matrix which is then multiplied by the term document matrix.  The resulting matrix product will have the same sparsity pattern of non-zero values as the input term document matrix.  As the product will clearly therefore also be a sparse matrix and will be the result of an arithmetic operation we should consider the CSR (Compressed Sparse Row) sparse matrix format for this matrix.  We should also consider converting the input matrix to CSR prior to the arithmetic operation.&lt;/p&gt;

&lt;p&gt;Here is a code snippet showing the current TF-IDF implementation using a Dense format matrix and a slice for the tf-idf weighting values.&lt;/p&gt;
type TfidfTransformer3 struct {
	weights []float64
}

func (t *TfidfTransformer3) Fit(mat mat64.Matrix) Transformer {
	m, n := mat.Dims()

	t.weights = make([]float64, m)

	for i := 0; i &lt; m; i++ {
		df := 0
		for j := 0; j &lt; n; j++ {
			if mat.At(i, j) != 0 {
				df++
			}
		}
		idf := math.Log(float64(1+n) / float64(1+df))
		t.weights[i] = idf
	}

	return t
}

func (t *TfidfTransformer3) Transform(mat mat64.Matrix) (*mat64.Dense, error) {
	m, n := mat.Dims()
	product := mat64.NewDense(m, n, nil)

	// apply a function to every element of the matrix in turn which
	// multiplies the element value by the corresponding term weight
	product.Apply(func(i, j int, v float64) float64 {
		return (v * t.weights[i])
	}, mat)

	return product, nil
}

func (t *TfidfTransformer3) FitTransform(mat mat64.Matrix) (*mat64.Dense, error) {
	return t.Fit(mat).Transform(mat)
}

&lt;p&gt;And here is the same code modified to use sparse format matrices for both the TF-IDF weighting values (DIAgonal format) and for the input and output matrices (CSR - Compressed Sparse Row format).&lt;/p&gt;
type SparseTfidfTransformer struct {
	transform mat64.Matrix
}

func (t *SparseTfidfTransformer) Fit(mat mat64.Matrix) *SparseTfidfTransformer {
	m, n := mat.Dims()

	weights := make([]float64, m)

	csr, isCsr := mat.(*sparse.CSR)

	for i := 0; i &lt; m; i++ {
		df := 0
		if isCsr {
			// if matrix is CSR, take advantage of the RowNNZ() method to 
			// get number of documents in which term appears.
			df = csr.RowNNZ(i)
		} else {
			for j := 0; j &lt; n; j++ {
				if mat.At(i, j) != 0 {
					df++
				}
			}
		}
		idf := math.Log(float64(1+n) / float64(1+df))
		weights[i] = idf
	}

	// build a diagonal matrix from array of term weighting values for
	// subsequent multiplication with term document matrics

	t.transform = sparse.NewDIA(m, weights)

	return t
}

func (t *SparseTfidfTransformer) Transform(mat mat64.Matrix) (mat64.Matrix, error) {
	product := &amp;sparse.CSR{}

	// simply multiply the matrix by our idf transform (the diagonal matrix 
	// of term weights)
	product.Mul(t.transform, mat)

	return product, nil
}

func (t *SparseTfidfTransformer) FitTransform(mat mat64.Matrix) (mat64.Matrix, error) {
	return t.Fit(mat).Transform(mat)
}

&lt;p&gt;This time, there is a little more difference between the 2 implementations.  If the input matrix is CSR format we take advantage of the RowNNZ() method to efficiently determine the number of non-zero values in each row which is used to calculate the inverse document frequency (the number of documents each term occurs in).  The TF-IDF weighting matrix is then multiplied by the input term document matrix.&lt;/p&gt;

&lt;p&gt;To test the two versions I used Go&amp;rsquo;s built in benchmark functionality and the same subset of the 20 Newsgroups dataset as used in Scikit Learn.  Here are the two benchmark functions.&lt;/p&gt;
func BenchmarkDenseTfidfFitTransform(b *testing.B) {
	files := Load(&#34;sci.space&#34;, &#34;sci.electronics&#34;)

	vect := NewCountVectoriser1(false)
	vect.Fit(files...)
	mat, _ := vect.Transform(files...)

	trans := &amp;TfidfTransformer3{}

	b.ResetTimer()
	for n := 0; n &lt; b.N; n++ {
		trans.FitTransform(mat)
	}
}

func BenchmarkSparseTfidfFitTransform(b *testing.B) {
	files := Load(&#34;sci.space&#34;, &#34;sci.electronics&#34;)

	vect := NewDOKCountVectoriser1(false)
	vect.Fit(files...)
	mat, _ := vect.Transform(files...)

	trans := &amp;SparseTfidfTransformer{}

	b.ResetTimer()
	for n := 0; n &lt; b.N; n++ {
		trans.FitTransform(mat.ToCSR())
	}
}

&lt;p&gt;The results are shown below.  It should be clear from the results that the new Sparse matrix implementation is considerably faster than the Dense matrix based implementation at 0.1 seconds vs 2.2 seconds but most importantly uses considerably less memory (16 MB vs the 500 MB used by the Dense implementation).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jamess-MacBook-Pro:nlpbench jbowman$ go test -bench=TfidfFitTransform -benchmem
BenchmarkDenseTfidfFitTransform-4   1	2238133519 ns/op	537378960 B/op	       4 allocs/op
BenchmarkSparseTfidfFitTransform-4  10	 138069814 ns/op	16253248 B/op	      12 allocs/op
PASS
ok  	github.com/james-bowman/nlpbench	8.277s
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-truncated-singular-value-decomposition&#34;&gt;3. Truncated Singular Value Decomposition&lt;/h3&gt;

&lt;p&gt;Step 3 outputs a matrix of much smaller dimensionality but as this matrix contains only the most significant dimensions, it is likely to be very dense.  Therefore, a dense matrix format is still the most appropriate choice of format for this step and so no changes will be made to this step at this point.&lt;/p&gt;

&lt;h2 id=&#34;putting-it-all-together&#34;&gt;Putting it all together&lt;/h2&gt;

&lt;p&gt;To test the impact of our changes to steps 1 &amp;amp; 2 above together we will run the following benchmark functions.&lt;/p&gt;
func BenchmarkDenseEndToEndVectAndTrans(b *testing.B) {
	files := Load(&#34;sci.space&#34;, &#34;sci.electronics&#34;)

	b.ResetTimer()

	vect := NewCountVectoriser1(false)
	trans := &amp;TfidfTransformer3{}

	for n := 0; n &lt; b.N; n++ {
		mat, _ := vect.FitTransform(files...)
		trans.FitTransform(mat)
	}
}

func BenchmarkSparseEndToEndVectAndTrans(b *testing.B) {
	files := Load(&#34;sci.space&#34;, &#34;sci.electronics&#34;)

	b.ResetTimer()

	vect := NewDOKCountVectoriser1(false)
	trans := &amp;SparseTfidfTransformer{}

	for n := 0; n &lt; b.N; n++ {
		mat, _ := vect.FitTransform(files...)
		trans.FitTransform(mat.ToCSR())
	}
}

&lt;p&gt;The results are shown below.  We can see that the current dense implementation of steps 1 &amp;amp; 2 (with the optimisations made in &lt;a href=&#34;http://www.jamesbowman.me/post/optimising-machine-learning-algorithms/&#34;&gt;the previous post in this series&lt;/a&gt;) took 3.3 seconds to complete and consumes over 1 GB of memory.  In contrast, the new sparse format based implementation takes only 1.2 seconds to complete and consumes only 150 MB of memory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jamess-MacBook-Pro:nlpbench jbowman$ go test -bench=EndToEndVect -benchmem
BenchmarkDenseEndToEndVectAndTrans-4    1	3373285711 ns/op	1180200344 B/op	 1379753 allocs/op
BenchmarkSparseEndToEndVectAndTrans-4   1	1232713457 ns/op	154805032 B/op	 1402819 allocs/op
PASS
ok  	github.com/james-bowman/nlpbench	5.110s
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;wrapping-up&#34;&gt;Wrapping Up&lt;/h2&gt;

&lt;p&gt;Using Sparse matrix formats in the place of Dense formats can significantly reduce both memory consumption and processing time.  Not necessarily because the algorithms are cleverer but simply because they are doing less work by processing only the non-zero elements.  Switching to sparse matrix formats, we saw a reduction in memory consumption and processing time from 1 GB to 150 MB and 3.3 seconds to 1.2 seconds respectively.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://github.com/james-bowman/sparse&#34;&gt;Golang sparse matrix format implementations used in this article are available on Github&lt;/a&gt; along with all the &lt;a href=&#34;http://github.com/james-bowman/nlpbench&#34;&gt;benchmarks and sample code&lt;/a&gt; used in this article.  The sparse matrix library is still quite basic in terms of features and available operations and I am hoping to extend it in due course with other operations (e.g. add, subtract, etc.) and further optimisations (e.g. parallel/fast matrix multiplication, BLAS integration, etc.).&lt;/p&gt;

&lt;p&gt;I would love to hear other people&amp;rsquo;s experiences of using or developing dense or sparse matrix implementations and any challenges they encountered and how they overcame them.  Please share your experiences, thoughts and suggestions in the comments below.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimising algorithms in Go for machine learning</title>
      <link>http://www.jamesbowman.me/post/optimising-machine-learning-algorithms/</link>
      <pubDate>Fri, 31 Mar 2017 16:31:29 +0100</pubDate>
      
      <guid>http://www.jamesbowman.me/post/optimising-machine-learning-algorithms/</guid>
      <description>

&lt;p&gt;In my &lt;a href=&#34;http://www.jamesbowman.me/post/semantic-analysis-of-webpages-with-machine-learning-in-go/&#34;&gt;last blog post I walked through the use of machine learning algorithms in Golang to analyse the latent semantic meaning of documents&lt;/a&gt;.  These algorithms, like many others in data science, rely on linear algebra and vector space analysis.  By their nature, they often have to deal with large data sets, so any inefficiencies in the data structures used or algorithms themselves can result in a large impact on overall performance and/or memory usage.  Inefficiencies that are negligable when working with small data sets can have a huge cost applied across extremely large datasets.  As memory is a constrained resource, this could end up limiting the size of data sets that may be processed (certainly without having to resort to persistent storage and/or alternative algorithms) or the types of algorithms used.  To this end, I decided to see if I could optimise the algorithms I used to consume less memory and improve processing performance without sacrificing too much functionality or accuracy.  This is the first in a series of articles sharing my experiences benchmarking and optimising the algorithms and data structures used whilst building out the &lt;a href=&#34;http://github.com/james-bowman/nlp&#34;&gt;nlp project&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;optimisation-of-the-tf-idf-transform&#34;&gt;Optimisation of the TF-IDF transform&lt;/h2&gt;

&lt;p&gt;As can be seen in the code snippet below, the TF-IDF transformer maintains a matrix internally,which is used to transform matrices containing raw term freqencies(tf) into weighted tf-idf values (term frequency - inverse document frequencies).  The transform is simply multiplied by the input tf matrix to produce the tf-idf matrix.  Please refer to the &lt;a href=&#34;http://www.jamesbowman.me/post/semantic-analysis-of-webpages-with-machine-learning-in-go/&#34;&gt;previous blog post for a more in depth description of TF-IDF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The transform matrix is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Diagonal_matrix#Symmetric_diagonal_matrices&#34;&gt;symmetric diagonal matrix&lt;/a&gt; and so will have as many rows and columns as there were unique terms/words in the training data set.  For a representative sample of 3000 articles from internet sites, I found the number of unique terms was typically around 30,000 which would require a 30,000 x 30,000 transform matrix.  Therefore the TF-IDF transform is a 30,000 x 30,000 matrix with elements of type float64 which will require approximately 7GB of memory.  Ironically the majority of elements within this matrix will contain zeros, in fact, only values along the diagonal (from top left to bottom right) will be non-zero.  There is clearly scope to optimise its memory use.&lt;/p&gt;
type TfidfTransformer1 struct {
    transform *mat64.Dense
}

func (t *TfidfTransformer1) Fit(mat mat64.Matrix) Transformer {
    m, n := mat.Dims()

    // build a diagonal matrix from array of term weighting values for subsequent
    // multiplication with term document matrics
    t.transform = mat64.NewDense(m, m, nil)

    for i := 0; i &lt; m; i++ {
        df := 0
        for j := 0; j &lt; n; j++ {
            if mat.At(i, j) != 0 {
                df++
            }
        }
        idf := math.Log(float64(1+n) / float64(1+df))
        t.transform.Set(i, i, idf)
    }

    return t
}

func (t *TfidfTransformer1) Transform(mat mat64.Matrix) (*mat64.Dense, error) {
    m, n := mat.Dims()
    product := mat64.NewDense(m, n, nil)

    // simply multiply the matrix by our idf transform (the diagonal matrix of term weights)
    product.Product(t.transform, mat)

    return product, nil
}

func (t *TfidfTransformer1) FitTransform(mat mat64.Matrix) (*mat64.Dense, error) {
    return t.Fit(mat).Transform(mat)
}

&lt;p&gt;My initial thought was to use a sparse matrix instead of the dense matrix implementation I was using.  A sparse matrix could more efficiently store a matrix mostly comprised of zeros, storing only the non-zero values.  Unfortunately the gonum matrix library I am using does not currently have a sparse matrix implementation.&lt;/p&gt;

&lt;p&gt;As an alternative approach, I considered simply storing the non-zero idf (term weights) values as an array of float64 values and then individually multiplying every element within the input raw tf matrix by its corresponding term weight from the array.  The code for this implementation called TfidfTransformer2 is shown below.  Note the differences in the &lt;code&gt;Fit()&lt;/code&gt; method where we no longer build a symmetric diagonal transform matrix and simply keep the array of term weights.  Also, the &lt;code&gt;Transform()&lt;/code&gt; method is now changed to simply iterate over every matrix element and multiply its value by the corresponding term weight from the array.&lt;/p&gt;
type TfidfTransformer2 struct {
    weights []float64
}

func (t *TfidfTransformer2) Fit(mat mat64.Matrix) Transformer {
    m, n := mat.Dims()

    // simply capture term weights as an array
    t.weights = make([]float64, m)

    for i := 0; i &lt; m; i++ {
        df := 0
        for j := 0; j &lt; n; j++ {
            if mat.At(i, j) != 0 {
                df++
            }
        }
        idf := math.Log(float64(1+n) / float64(1+df))
        t.weights[i] = idf
    }

    return t
}

func (t *TfidfTransformer2) Transform(mat mat64.Matrix) (*mat64.Dense, error) {
    m, n := mat.Dims()
    product := mat64.NewDense(m, n, nil)

    // iterate over every element of the matrix in turn and 
    // multiply the element value by the corresponding term weight
    for i := 0; i &lt; m; i++ {
        for j := 0; j &lt; n; j++ {
            product.Set(i, j, mat.At(i, j) * t.weights[i])
        }
    }

    return product, nil
}

func (t *TfidfTransformer2) FitTransform(mat mat64.Matrix) (*mat64.Dense, error) {
    return t.Fit(mat).Transform(mat)
}

&lt;p&gt;This second approach would be considerably more memory efficient but I had some concerns over performance, specifically how well iterating over every matrix element, using nested loops, outside of the heavily optimised LAPACK/BLAS libraries would perform.&lt;/p&gt;

&lt;p&gt;Then I found a matrix &lt;code&gt;Apply()&lt;/code&gt; method within the gonum/mat64 library that would apply a function to every element within the matrix.  Unlike the nested loop approach, this would allow the optimised LAPACK/BLAS libraries to handle the iteration so should theoretically be more performant.  This implementation would be identical to the &lt;code&gt;TfidfTransformer2&lt;/code&gt; above save for the &lt;code&gt;Transform()&lt;/code&gt; method where the matrix &lt;code&gt;Apply()&lt;/code&gt; method is used instead of nested loops.  The differing &lt;code&gt;Transform()&lt;/code&gt; method is shown below.&lt;/p&gt;
func (t *TfidfTransformer3) Transform(mat mat64.Matrix) (*mat64.Dense, error) {
    m, n := mat.Dims()
    product := mat64.NewDense(m, n, nil)

    // apply a function to every element of the matrix in turn which
    // multiplies the element value by the corresponding term weight
    product.Apply(func(i, j int, v float64) float64 {
        return (v * t.weights[i])
    }, mat)

    return product, nil
}

&lt;h3 id=&#34;benchmarks&#34;&gt;Benchmarks&lt;/h3&gt;

&lt;p&gt;To confirm our hypothesis that the new implementations will outperform the current matrix transform based implementation we will run some benchmarks using the inbuilt benchmark functionality inside Go&amp;rsquo;s &lt;code&gt;testing&lt;/code&gt; package.&lt;/p&gt;

&lt;p&gt;Implementing a benchmark in Go is much the same as implementing a test except the function name is prefixed with &lt;code&gt;Benchmark&lt;/code&gt; rather than &lt;code&gt;Test&lt;/code&gt;.  Here are separate benchmark implementations for the &lt;code&gt;Fit()&lt;/code&gt;, &lt;code&gt;Transform()&lt;/code&gt; and &lt;code&gt;FitTransform()&lt;/code&gt; methods exercised for each of the 3 algorithm implementations.  As the &lt;code&gt;Fit()&lt;/code&gt; method is identical between implementation 2 (TfidfTransformer2) and 3 (TfidfTransformer3) I have not bothered to benchmark both instead just benchmarking the &lt;code&gt;Fit()&lt;/code&gt; method from implementation 2.&lt;/p&gt;
func benchmarkFit(t Transformer, m, n int, b *testing.B) {
    mat := mat64.NewDense(m, n, nil)

    b.ResetTimer()
    for n := 0; n &lt; b.N; n++ {
        t.Fit(mat)
    }
}

func benchmarkFitTransform(t Transformer, m, n int, b *testing.B) {
    mat := mat64.NewDense(m, n, nil)

    b.ResetTimer()
    for n := 0; n &lt; b.N; n++ {
        t.FitTransform(mat)
    }
}

func benchmarkTransform(t Transformer, m, n int, b *testing.B) {
    mat := mat64.NewDense(m, n, nil)
    t.Fit(mat)

    b.ResetTimer()
    for n := 0; n &lt; b.N; n++ {
        t.Transform(mat)
    }
}

func BenchmarkTFIDF1Fit30000x3000(b *testing.B) {
    benchmarkFit(&amp;TfidfTransformer1{}, 30000, 3000, b)
}
func BenchmarkTFIDF1Transform30000x3000(b *testing.B) {
    benchmarkTransform(&amp;TfidfTransformer1{}, 30000, 3000, b)
}
func BenchmarkTFIDF1FitTransform30000x3000(b *testing.B) {
    benchmarkFitTransform(&amp;TfidfTransformer1{}, 30000, 3000, b)
}
func BenchmarkTFIDF2Fit30000x3000(b *testing.B) {
    benchmarkFit(&amp;TfidfTransformer2{}, 30000, 3000, b)
}
func BenchmarkTFIDF2Transform30000x3000(b *testing.B) {
    benchmarkTransform(&amp;TfidfTransformer2{}, 30000, 3000, b)
}
func BenchmarkTFIDF2FitTransform30000x3000(b *testing.B) {
    benchmarkFitTransform(&amp;TfidfTransformer2{}, 30000, 3000, b)
}
func BenchmarkTFIDF3Transform30000x3000(b *testing.B) {
    benchmarkTransform(&amp;TfidfTransformer3{}, 30000, 3000, b)
}
func BenchmarkTFIDF3FitTransform30000x3000(b *testing.B) {
    benchmarkFitTransform(&amp;TfidfTransformer3{}, 30000, 3000, b)
}

&lt;p&gt;The benchmarks are run in the same way as tests except that the &lt;code&gt;-bench&lt;/code&gt; option is used to express a regular expression matching the names of benchmark functions to run.  The regular expression &lt;code&gt;.&lt;/code&gt; will match all benchmarks or as in the output below, &lt;code&gt;30000&lt;/code&gt; will match and run all benchmarks with 30000 in the function name.  The &lt;code&gt;-benchmem&lt;/code&gt; option will output memory allocations during the benchmark.  Here are the results of the benchmarks.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go test -bench=30000 -benchmem
BenchmarkTFIDF1Fit30000x3000-4              1  6025208569 ns/op     7200006208 B/op     2 allocs/op
BenchmarkTFIDF1Transform30000x3000-4        1  41773814068 ns/op    720005856 B/op      18 allocs/op
BenchmarkTFIDF1FitTransform30000x3000-4     1  50079258438 ns/op    7920011056 B/op     11 allocs/op
BenchmarkTFIDF2Fit30000x3000-4              3  513875302 ns/op      245760 B/op         1 allocs/op
BenchmarkTFIDF2Transform30000x3000-4        1  1669005488 ns/op     720003136 B/op      2 allocs/op
BenchmarkTFIDF2FitTransform30000x3000-4     1  1472562820 ns/op     720250752 B/op      6 allocs/op
BenchmarkTFIDF3Transform30000x3000-4        2  884655455 ns/op      720003136 B/op      2 allocs/op
BenchmarkTFIDF3FitTransform30000x3000-4     1  1029592374 ns/op     720248896 B/op      3 allocs/op
PASS
ok      github.com/james-bowman/nlpbench    121.167s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From the benchmark results, we can see that as we expected, the &lt;code&gt;Fit()&lt;/code&gt; method on the original implementation is allocating around 7 GB of memory (first row).  In comparison we can see that the new array based &lt;code&gt;Fit()&lt;/code&gt; implementation is allocating around 250 KB.  This is a significant improvement, but how does the performance stack up?&lt;/p&gt;

&lt;p&gt;According to the benchmark, the original implementation took 41773814068 nano seconds to perform the transform (second row).  Comparing this with 1669005488 nano seconds for the second nested loop based implementation (TfidfTransformer2 - row 5) would suggest the nested loop implementation is around 20 times faster.  If we further compare the third implementation (TfidfTransformer3), using the &lt;code&gt;Apply()&lt;/code&gt; method and delegating the iteration over matrix elements down into the optimised LAPACK/BLAS libraries (row 7) we can see this is about twice as fast as the second implementation executing in around half the time.  As we would expect,memory usage is the same for both implementation 2 and 3.&lt;/p&gt;

&lt;h2 id=&#34;wrapping-up&#34;&gt;Wrapping up&lt;/h2&gt;

&lt;p&gt;We started using a standard implementation of the TF-IDF algorithm (albeit using a Dense matrix implementation) and explored possible optimisations for memory usage.  We looked at 2 alternative methods and then tested our hypothesis using Go&amp;rsquo;s inbuilt benchmark functionality.  The results showed that our optimisations materially improved not just memory consumption but also performance.  For a dataset with 30,000 terms across 3,000 documents, we showed the original implementation executed in around 41 seconds, consuming around 7 GB.  The new implementation processes the same dataset in just 0.8 seconds, consuming around 250 KB of memory.&lt;/p&gt;

&lt;p&gt;All &lt;a href=&#34;http://github.com/james-bowman/nlpbench&#34;&gt;the Golang benchmarking code used in this article is on Github&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>