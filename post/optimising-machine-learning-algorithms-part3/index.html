<!DOCTYPE html>
<html lang="en-gb">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    


    
    
        
        <meta name="twitter:card" content="summary_large_image"/>
        <meta name="twitter:image" content="http://www.jamesbowman.me//post/juggler-1216853_1280.jpg"/>
    
    



<meta name="twitter:title" content="Optimising algorithms in Go for machine learning - Part 3: The hashing trick"/>
<meta name="twitter:description" content="Using feature hashing to avoid training vocabularies in Golang for Natural Language Processing (NLP) and machine learning"/>
<meta name="twitter:site" content="@jamesebowman"/>



  	<meta property="og:title" content="Optimising algorithms in Go for machine learning - Part 3: The hashing trick &middot; James Bowman" />
  	<meta property="og:site_name" content="James Bowman" />
  	<meta property="og:url" content="http://www.jamesbowman.me/post/optimising-machine-learning-algorithms-part3/" />

    
    <meta property="og:description" content="Using feature hashing to avoid training vocabularies in Golang for Natural Language Processing (NLP) and machine learning" />
  	<meta property="og:type" content="article" />
    
    <meta property="og:image" content="http://www.jamesbowman.me//post/juggler-1216853_1280.jpg" />
    
    <meta property="article:published_time" content="2017-07-07T18:45:40&#43;01:00" />

    
    <meta property="article:tag" content="development" />
    
    <meta property="article:tag" content="machine learning" />
    
    <meta property="article:tag" content="go" />
    
    <meta property="article:tag" content="algorithms" />
    
    

    
  
    <title>Optimising algorithms in Go for machine learning - Part 3: The hashing trick &middot; James Bowman</title>

    
    <meta name="description" content="Using feature hashing to avoid training vocabularies in Golang for Natural Language Processing (NLP) and machine learning" />
    

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="http://www.jamesbowman.me/images/favicon.ico">
	  <link rel="apple-touch-icon" href="http://www.jamesbowman.me/images/apple-touch-icon.png" />

    <link rel="stylesheet" type="text/css" href="http://www.jamesbowman.me/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="http://www.jamesbowman.me/css/nav.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata" />


    
        <link href="http://feeds.jamesbowman.me/jamesbowman/blog" rel="alternate" type="application/rss+xml" title="James Bowman" />
    
    <meta name="generator" content="Hugo 0.18" />

    <link rel="canonical" href="http://www.jamesbowman.me/post/optimising-machine-learning-algorithms-part3/" />

    
      
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "James Bowman",
        "logo": "http://www.jamesbowman.me/selfportraitBWcirc.png"
    },
    "author": {
        "@type": "Person",
        "name": "James Bowman",
        
        "image": {
            "@type": "ImageObject",
            "url": "http://www.jamesbowman.me/selfportraitBWcirc.png",
            "width": 250,
            "height": 250
        }, 
        
        "url": "http://www.jamesbowman.me",
        "sameAs": [
            
            
             "https://plus.google.com/+JamesBowman1978",
             "https://github.com/james-bowman",
             
             "https://uk.linkedin.com/in/jamesedwardbowman",
             "https://twitter.com/JameseBowman"
            
        ],
        "description": "I love coding in Go, microservices, continuous delivery and DevOps. I am fascinated by coaching, learning, people's behaviour and organisational change."
        
    },
    "headline": "Optimising algorithms in Go for machine learning - Part 3: The hashing trick",
    "name": "Optimising algorithms in Go for machine learning - Part 3: The hashing trick",
    "wordCount":  1525 ,
    "timeRequired": "PT8M",
    "inLanguage": {
      "@type": "Language",
      "alternateName": "en"
    },
    "url": "http://www.jamesbowman.me/post/optimising-machine-learning-algorithms-part3/",
    "datePublished": "2017-07-07T18:45Z",
    "dateModified": "2017-07-07T18:45Z",
    
    "image": {
        "@type": "ImageObject",
        "url": "http://www.jamesbowman.me//post/juggler-1216853_1280.jpg",
        "width": 3000,
        "height": 1445
    },
    
    
    "keywords": "development, machine learning, go, algorithms",
    "description": "Using feature hashing to avoid training vocabularies in Golang for Natural Language Processing (NLP) and machine learning",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://www.jamesbowman.me/post/optimising-machine-learning-algorithms-part3/"
    }
}
    </script>
    


    

    

    
        
<script>
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-MRG66N');</script>



    
</head>
<body class="nav-closed">

  
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRG66N"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
        
        
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/">Blog</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/post">Archive</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/talks">Talks</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/about-james-bowman">About me</a>
            </li>
        
    </ul>
    
    
    <a class="subscribe-button icon-feed" href="http://feeds.jamesbowman.me/jamesbowman/blog">Subscribe</a> </div>
    
</div>
<span class="nav-cover"></span>


 <div class="site-wrapper">




  
  <header class="main-header post-head" style="background-image: url(http://www.jamesbowman.me//post/juggler-1216853_1280.jpg)">
  
  <nav class="main-nav overlay clearfix">


  <a class="menu-button" style="float: left;" href="http://www.jamesbowman.me/"><span class="icon-home"/><span class="word">&lt;&nbsp; Home</span></a>
  
      <a class="menu-button" href="#"><span class="burger">&#9776;</span><span class="word">Menu</span></a>
  
  </nav>
</header>



<main class="content" role="main">




  <article class="post post">

    <header class="post-header">
        <h1 class="post-title">Optimising algorithms in Go for machine learning - Part 3: The hashing trick</h1>
        <small>Using feature hashing to avoid training vocabularies in Golang for Natural Language Processing (NLP) and machine learning</small>

        <section class="post-meta">
        
          <time class="post-date" datetime="2017-07-07T18:45:40&#43;01:00">
            Jul 7, 2017
          </time>
        
         
          <span class="post-tag small"><a href="http://www.jamesbowman.me/tags/development/">#development</a></span>
         
          <span class="post-tag small"><a href="http://www.jamesbowman.me/tags/machine-learning/">#machine learning</a></span>
         
          <span class="post-tag small"><a href="http://www.jamesbowman.me/tags/go/">#go</a></span>
         
          <span class="post-tag small"><a href="http://www.jamesbowman.me/tags/algorithms/">#algorithms</a></span>
         
        </section>
    </header>

    <section class="post-content">
      

<p>This is the third in a series of blog posts sharing my experiences working with algorithms and data structures for machine learning.  These experiences were gained whilst building out the <a href="http://github.com/james-bowman/nlp">nlp project</a> for <a href="http://www.jamesbowman.me/post/semantic-analysis-of-webpages-with-machine-learning-in-go/">LSA (Latent Semantic Analysis)</a> of text documents.</p>

<p>In Part 2 of this series, I explored <a href="http://www.jamesbowman.me/post/optimising-machine-learning-algorithms-part2/">sparse matrix formats</a> as a set of data structures for more efficiently storing and manipulating sparsely populated matrices (matrices where most elements contain zero values).  We tested the impact of using sparse formats, over the originally implemented dense matrix formats, using Go&rsquo;s inbuilt benchmark functionality and found that our optimisations led to a reduction in memory consumption and processing time from 1 GB to 150 MB and 3.3 seconds to 1.2 seconds respectively.</p>

<p>The <a href="http://github.com/james-bowman/sparse">Golang sparse matrix format implementations</a> used in the article are available on <a href="http://github.com/james-bowman/sparse">Github</a> along with all the <a href="http://github.com/james-bowman/nlpbench">benchmarks and sample code</a> used in this series.</p>

<p>In this article we will use the sparse matrix formats from the previous article to implement &lsquo;the hashing trick&rsquo;.</p>

<h2 id="feature-extraction-vectorisation">Feature Extraction (Vectorisation)</h2>

<p>In machine learning applications we frequently encode &lsquo;things&rsquo; as vectors of numerical features to allow quantative analysis and comparison.  For natural language processing, these &lsquo;things&rsquo; are usually text documents encoded as vectors of term frequencies i.e. the frequency with which each unique word (term) appears in the document.  When vectorising features in this way, it is important that all vectors are encoded consistently i.e. that frequencies for each specific term always map to the same row from one vector to another.  If featurers were not encoded consistently, with different positions in vectors or vectors of different length, then they could not be compared to one another: it would be like comparing apples and oranges.</p>

<p>Usually, consistent vectorisation is achieved using a predefined vocabulary of words, with each unique word in the vocabulary corresponding to the same specific vector row.  This can be implemented using a dictionary/map structure to map each unique word of the vocabulary to a vector row index.  The vocabulary can either be manually created, or more usually, it is trained from a representative set of training documents.  This approach has 3 main disadvantages as follows:</p>

<ol>
<li>The memory required to store the vocabulary and associated row index mappings (especially as the vocabulary gets large - see point 2 below)</li>
<li>Any words missing from the vocabulary will not be represented as features in vectors encoded from the vocabulary.  To overcome this limitation, we usually train the vocabulary with a large and fully representative training data set to increase the chances that the model is exposed to any and all potentially required terms during training.</li>
<li>The requirement to train the vocabulary can make this algorithm unsuitable for some streaming data applications.</li>
</ol>

<h2 id="feature-hashing-the-hashing-trick">Feature Hashing (&lsquo;the hashing trick&rsquo;)</h2>

<p>Feature hashing (also known as &lsquo;the hashing trick&rsquo;) uses a hash function to map values to indices in feature vectors.  This removes the reliance on a pre-defined or trained vocabulary, as features are directly mapped based upon their hash value, meaning the same index can be repeatedly and consistenly reproduced given the same word and hash function.  This has the effect of reducing memory requirements and making it highly suitable for use in streaming applications where a pre-defined vocabulary might not be available or practical.</p>

<p>When using feature hashinig, the number of unique words across the corpus is unknown, so the dimensionality of feature vectors must be set in advance (so all vectors are in the same dimensional space).  This length is entirely arbitary but should be sufficiently large to support all required words from the language being used.  For an English language document corpora, this number is likely to be around 1,000,000 although in many applications, much smaller values can be sufficient.</p>

<p>Once the vector dimensionality is fixed, we can apply a hash function to each word in the documents to be vectorised.  We then calculate the modulus of the resulting hash value with the vector length.  This calculation will yield the row index for the word applicable for all vectors within the defined dimensional space (using the same hash function)  - <code>index = hash(word) % vectorLength</code>.</p>

<p>It should be clear that this approach will create vectors of extremely high dimensionality and so without <a href="http://www.jamesbowman.me/post/optimising-machine-learning-algorithms-part2/">sparse matrix formats</a>, as discussed in the previous article, this approach is not feasible because of the memory requirements necessary to store all the elements in dense matrix format.</p>

<p>One potential downside of using the hashing trick is that multiple, different words may translate to the same index in the vector space (hash collisions) resulting in a loss of precision.  One way of reducing the effect of collisions is to choose a dimensionality sufficiently high enough so as to lessen the likelihood of collisions.  Another approach sometimes adopted is to use a second hash function that will indicate the sign (+/-) to apply to the value being updated within the vector.  If 2 words equate to the same index in the vector space - one may result in +1 (or +<em>n</em> where <em>n</em> is the number of occurances of the word within the corresponding document) and the other may result in -1 (or -<em>n</em> where <em>n</em> is the number of occurances of the word within the corresponding document).  In this way, the collision does not compound the error but rather the two word occurances cancel each other out.</p>

<p>Another disadvantage of feature hashing is that, once vectors are created, it is difficult to determine which frequency values relate to which terms.  When using a vocabulary, with associated index mappings, it is easier to analyse frequencies for particular words.  This can make feature hashing unsuitable for some applications.</p>

<h2 id="implementation">Implementation</h2>

<p>Here is the code for the <code>HashingVectoriser</code> that implements the hashing trick.</p>
type HashingVectoriser struct {
	NumFeatures   int
	wordTokeniser *regexp.Regexp
	stopWords     map[string]bool
}

func NewHashingVectoriser(removeStopwords bool, numFeatures int) *HashingVectoriser {
	var stop map[string]bool

	if removeStopwords {
		stop = make(map[string]bool)
		for _, word := range stopWords {
			stop[word] = true
		}
	}
	return &HashingVectoriser{
		NumFeatures:   numFeatures,
		wordTokeniser: regexp.MustCompile("\\w+"),
		stopWords:     stop,
	}
}

func (v *HashingVectoriser) Fit(train ...string) *HashingVectoriser {
	// Do nothing - the HashingVectoriser is stateless and does not
	// require training.  Method included for compatibility with
	// other vectorisers
	return v
}

func (v *HashingVectoriser) Transform(docs ...string) (mat64.Matrix, error) {
	mat := sparse.NewDOK(v.NumFeatures, len(docs))

	for d, doc := range docs {
		words := v.tokenise(doc)

		for _, word := range words {
			if v.stopWords != nil {
				if v.stopWords[word] {
					continue
				}
			}
			h := murmur3.Sum32([]byte(word))
			i := int(h) % v.NumFeatures

			mat.Set(i, d, mat.At(i, d)+1)
		}
	}
	return mat, nil
}

func (v *HashingVectoriser) FitTransform(docs ...string) (mat64.Matrix, error) {
	return v.Transform(docs...)
}

func (v *HashingVectoriser) tokenise(text string) []string {
	// convert content to lower case
	c := strings.ToLower(text)

	// match whole words, removing any punctuation/whitespace
	words := v.wordTokeniser.FindAllString(c, -1)

	return words
}

<p>We shall do some basic benchmarking of the <code>HashingVectoriser</code> against our exising <code>CountVectoriser</code> implementation.  Whilst the main advantage of the HashingVectoriser is that it removes the dependency on the training data set to train the model with a complete vocabulary, we want to ensure the performance and memory consumption are at least comparable to the <code>CountVectoriser</code>.</p>

<p>We shall benchmark the <code>Transform()</code> method of the <code>HashingVectoriser</code> and compare this against benchmarks for both the <code>Transform()</code> and <code>FitTransform()</code> methods of <code>CountVectoriser</code>.  This will allow comparison of the raw transforms for both algorithms but also to see any savings achieved by not having to train the vocabulary (Fitting).</p>

<p>Here is the benchmarking code which uses <a href="https://golang.org/pkg/testing/#hdr-Benchmarks">Golang&rsquo;s inbuilt benchmarking functionality</a>.  The benchmarks make use of the 20 Newsgroups dataset as used in Scikit Learn.  In this case, we are using the entire dataset rather than a subset as in the last article.  This dataset contains 19,998 documents comprising a vocabulary of 209,412 unique words.  Our CountVectoriser will create a term document matrix 209,412 rows x 19,998 columns and a density of 0.1%.  We shall set the vector dimensionality for the <code>HashingVectoriser</code> to 1,000,000 rows.</p>
// Benchmark CountVectoriser vs HashingVectoriser
func BenchmarkNLPCountVectoriserTransform(b *testing.B) {
	files := Load()
	vect := nlp.NewCountVectoriser(false)
	vect.Fit(files...)

	b.ResetTimer()
	for n := 0; n < b.N; n++ {
		mat, _ := vect.Transform(files...)
	}
}

func BenchmarkNLPCountVectoriserFitTransform(b *testing.B) {
	files := Load()
	vect := nlp.NewCountVectoriser(false)

	b.ResetTimer()
	for n := 0; n < b.N; n++ {
		mat, _ := vect.FitTransform(files...)
	}
}

func BenchmarkNLPHashingVectoriserFitTransform(b *testing.B) {
	files := Load()
	vect := nlp.NewHashingVectoriser(false, 1000000)

	b.ResetTimer()
	for n := 0; n < b.N; n++ {
		mat, _ := vect.FitTransform(files...)
	}
}

<p>And here are the results.</p>

<pre><code>Jamess-MacBook-Pro:nlpbench jbowman$ go test -bench=NLP -benchmem
BenchmarkNLPCountVectoriserTransform-4       1	7364512097 ns/op	1098524336 B/op	 8181853 allocs/op
BenchmarkNLPCountVectoriserFitTransform-4    1	11587444509 ns/op	1717760288 B/op	16137998 allocs/op
BenchmarkNLPHashingVectoriserFitTransform-4  1	6578351432 ns/op	1098525672 B/op	 8182272 allocs/op
PASS
ok  	github.com/james-bowman/nlpbench	35.133s
</code></pre>

<p>From the results of the benchmarks we can see that the <code>HashingVectoriser</code> is slightly faster at transforming the data than the <code>CountVectoriser</code>.  However, the real saving is that the <code>HashingVectoriser</code> does not require training.  This can save time - using the same sized corpus for training and transforming, both take similar lengths of time - but also memory consumption.  The <code>HashingVectoriser</code> does not need to store the vocabulary and associated index mappings in memory (over 600 MB using our training data).</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>Using a <code>HashingVectoriser</code> can remove the need to pre-train a vocabulary better supporting streaming applications and also reducing memory consumption (by over 600 MB in our benchmarks).  However, the <code>HashingVectoriser</code> is not without its drawbacks.  Using a <code>HashingVectoriser</code>, one trades precision and accuracy for utility and memory consumption.</p>

<p>The <a href="http://github.com/james-bowman/nlpbench">benchmarks and sample code</a> used in this article are all on <a href="http://github.com/james-bowman/nlpbench">Github</a>.</p>


       
    </section>


  <footer class="post-footer">


    
    <figure class="author-image">
        <a class="img" href="http://www.jamesbowman.me/" style="background-image: url(http://www.jamesbowman.me/selfportraitBWcirc.png)"><span class="hidden">James Bowman's Picture</span></a>
    </figure>
    

    





<section class="author">
  <h4><a href="http://www.jamesbowman.me/">James Bowman</a></h4>
  
  <p>I love coding in Go, microservices, continuous delivery and DevOps. I am fascinated by coaching, learning, people&#39;s behaviour and organisational change.</p>
  
  <div class="author-meta">
    <span class="author-location icon-location">London, UK</span>
    <span class="author-link icon-link"><a href="http://www.jamesbowman.me">http://www.jamesbowman.me</a></span>
  </div>
</section>



    
<section class="share">
  <h4>Share this post</h4>
  <a class="icon-twitter" style="font-size: 1.4em" href="https://twitter.com/share?text=Optimising%20algorithms%20in%20Go%20for%20machine%20learning%20-%20Part%203%3a%20The%20hashing%20trick&nbsp;-&nbsp;James%20Bowman&amp;url=http%3a%2f%2fwww.jamesbowman.me%2fpost%2foptimising-machine-learning-algorithms-part3%2f"
      onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
      <span class="hidden">Twitter</span>
  </a>
  <a class="icon-facebook" style="font-size: 1.4em" href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2fwww.jamesbowman.me%2fpost%2foptimising-machine-learning-algorithms-part3%2f"
      onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
      <span class="hidden">Facebook</span>
  </a>
  <a class="icon-pinterest" style="font-size: 1.4em" href="http://pinterest.com/pin/create/button/?url=http%3a%2f%2fwww.jamesbowman.me%2fpost%2foptimising-machine-learning-algorithms-part3%2f&amp;description=Optimising%20algorithms%20in%20Go%20for%20machine%20learning%20-%20Part%203%3a%20The%20hashing%20trick"
      onclick="window.open(this.href, 'pinterest-share','width=580,height=296');return false;">
      <span class="hidden">Pinterest</span>
  </a>
  <a class="icon-google-plus" style="font-size: 1.4em" href="https://plus.google.com/share?url=http%3a%2f%2fwww.jamesbowman.me%2fpost%2foptimising-machine-learning-algorithms-part3%2f"
     onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
      <span class="hidden">Google+</span>
  </a>
</section>



    

<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'jamesebowman';
  var disqus_url = 'http:\/\/www.jamesbowman.me\/post\/optimising-machine-learning-algorithms-part3\/';
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




  </footer>
</article>

</main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">James Bowman</a> All rights reserved 2017</section>
        
        <section class="poweredby">Proudly generated by <a class="icon-hugo" href="http://gohugo.io">HUGO</a>, with <a class="icon-theme" href="https://github.com/vjeantet/hugo-theme-casper">Casper</a> theme</section>
        
    </footer>
    </div>
    <script type="text/javascript" src="http://www.jamesbowman.me/js/jquery.js"></script>
    <script type="text/javascript" src="http://www.jamesbowman.me/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="http://www.jamesbowman.me/js/index.js"></script>
    
        
<script type="text/javascript" src="//newsharecounts.s3-us-west-2.amazonaws.com/nsc.js"></script>

    
</body>
</html>

